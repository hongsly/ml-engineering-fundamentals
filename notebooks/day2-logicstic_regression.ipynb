{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2638e7-4ffb-4f28-aa81-f73318009e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1e-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f96032-2165-4e84-a8a9-74b925ef9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EPSILON = 1e-10\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w = None  # (m,)\n",
    "        self.b = None\n",
    "\n",
    "    # X: (n, m)\n",
    "    # return: (n,)\n",
    "    def predict(self, X):\n",
    "        if self.w is None:\n",
    "            raise ValueError(\"not initizlied\")\n",
    "        z = X @ self.w + self.b\n",
    "        z = np.clip(z, -500,500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict_classes(self, X):\n",
    "        y = self.predict(X)\n",
    "        return (y > 0.5).astype(int)\n",
    "\n",
    "    # X: (n, m)\n",
    "    # y: (n,)\n",
    "    def loss(self, X, y):\n",
    "        n = len(X)\n",
    "        y_pred = self.predict(X)\n",
    "        # clip for numerical stability\n",
    "        y_pred = np.clip(y_pred, EPSILON, 1-EPSILON)\n",
    "        return np.sum(-(y * np.log(y_pred) + (1-y) * np.log(1-y_pred))) / n\n",
    "\n",
    "    def gradient_descent(self, X, y, learning_rate):\n",
    "        n = len(X)\n",
    "        y_pred = self.predict(X)\n",
    "        # (m,) = (m, n) @ (n,)\n",
    "        dldw = X.T @ (y_pred - y) / n\n",
    "        dldb = np.sum(y_pred - y) / n\n",
    "        self.w -= dldw * learning_rate\n",
    "        self.b -= dldb * learning_rate\n",
    "\n",
    "    def fit(self, X, y, steps, learning_rate=0.01):\n",
    "        n = len(X)\n",
    "        m = len(X[0])\n",
    "        self.w = np.random.randn(m)\n",
    "        self.b = 0\n",
    "        \n",
    "        for i in range(steps):\n",
    "            self.gradient_descent(X, y, learning_rate)\n",
    "            if i % 50 == 0:\n",
    "                loss = self.loss(X, y)\n",
    "                print(f\"Step {i}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa683f-b5b3-42bf-862a-225bf21a8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3], [3,1,4]])\n",
    "y = np.array([0,1])\n",
    "l = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a700e-e2dc-4cc1-9470-7455e64fa6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.fit(X, y, steps=801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2910f-ac31-48fd-a97f-a9166578a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857dd1d9-db73-40b3-8237-ae087b74e0f0",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d816b9e-c1a3-4b37-8c29-d05d8219f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Test on a simple dataset\n",
    "  from sklearn.datasets import make_classification\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "  X, y = make_classification(n_samples=1000, n_features=10, n_informative=8,\n",
    "                             n_redundant=2, n_classes=2, random_state=42)\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Add normalization\n",
    "  scaler = StandardScaler()\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_test = scaler.transform(X_test)\n",
    "\n",
    "  # Train your model\n",
    "  model = LogisticRegression()\n",
    "  model.fit(X_train, y_train, steps=1000, learning_rate=0.01)\n",
    "\n",
    "  # Evaluate\n",
    "  y_pred_probs = model.predict(X_test)\n",
    "  y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "  accuracy = np.mean(y_pred == y_test)\n",
    "  print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "  # Compare with sklearn\n",
    "  from sklearn.linear_model import LogisticRegression as SklearnLR\n",
    "  sklearn_model = SklearnLR()\n",
    "  sklearn_model.fit(X_train, y_train)\n",
    "  sklearn_acc = sklearn_model.score(X_test, y_test)\n",
    "  print(f\"Sklearn Accuracy: {sklearn_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc791a1-427e-4a57-b658-45d0efbb9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "  import numpy as np\n",
    "  from sklearn.datasets import load_breast_cancer\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "  # Use a real, clean dataset\n",
    "  data = load_breast_cancer()\n",
    "  X, y = data.data, data.target\n",
    "\n",
    "  # Split\n",
    "  X_train, X_test, y_train, y_test = train_test_split(\n",
    "      X, y, test_size=0.2, random_state=42\n",
    "  )\n",
    "\n",
    "  # Normalize\n",
    "  scaler = StandardScaler()\n",
    "  X_train = scaler.fit_transform(X_train)\n",
    "  X_test = scaler.transform(X_test)\n",
    "\n",
    "  # Verify normalization worked\n",
    "  print(f\"X_train mean: {X_train.mean():.6f} (should be â‰ˆ 0)\")\n",
    "  print(f\"X_train std: {X_train.std():.6f} (should be â‰ˆ 1)\")\n",
    "\n",
    "  # Train your model\n",
    "  model = LogisticRegression()\n",
    "  model.fit(X_train, y_train, steps=500, learning_rate=0.1)\n",
    "\n",
    "  # Test\n",
    "  y_pred = model.predict_classes(X_test)\n",
    "  accuracy = np.mean(y_pred == y_test)\n",
    "  print(f\"\\nYour Model Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "  # Compare with sklearn\n",
    "  from sklearn.linear_model import LogisticRegression as SklearnLR\n",
    "  sklearn_model = SklearnLR(max_iter=500)\n",
    "  sklearn_model.fit(X_train, y_train)\n",
    "  sklearn_acc = sklearn_model.score(X_test, y_test)\n",
    "  print(f\"Sklearn Accuracy: {sklearn_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349a4f9-10ff-417c-ab1d-bbd8a24b4856",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb339c-63d6-4365-b38c-6ce1682a1960",
   "metadata": {},
   "source": [
    "ðŸŸ¡ Minor Improvements (Not critical, but good to know)\n",
    "\n",
    "  1. Weight Initialization\n",
    "\n",
    "  Currently:\n",
    "  ```\n",
    "  self.w = np.random.randn(m)  # Random normal\n",
    "  ```\n",
    "  Better for logistic regression:\n",
    "  ```\n",
    "  self.w = np.zeros(m)  # Or small random: np.random.randn(m) * 0.01\n",
    "  ```\n",
    "\n",
    "  Why:\n",
    "  - **Logistic regression typically starts at zeros or very small values**\n",
    "  - Large random weights can cause sigmoid saturation early\n",
    "  - But your current approach works fine for simple cases!\n",
    "\n",
    "  2. Return Predicted Classes\n",
    "\n",
    "  Add a method to get binary predictions:\n",
    "  ```\n",
    "  def predict_classes(self, X, threshold=0.5):\n",
    "      \"\"\"Return binary class predictions (0 or 1)\"\"\"\n",
    "      probabilities = self.predict(X)\n",
    "      return (probabilities >= threshold).astype(int)\n",
    "  ```\n",
    "\n",
    "  This is useful because:\n",
    "  - predict() returns probabilities [0, 1]\n",
    "  - Often we need binary classifications {0, 1}\n",
    "  - Interviewers might ask: \"How do I get actual class predictions?\"\n",
    "\n",
    "  3. Consider Gradient Clipping (Optional)\n",
    "\n",
    "  For numerical stability in gradient_descent:\n",
    "  ```\n",
    "  dldw = np.clip(X.T @ (y_pred - y) / n, -1, 1)  # Prevent exploding gradients\n",
    "  ```\n",
    "\n",
    "  But this is optional - your current version is fine!\n",
    "\n",
    "  4. Method Naming (Very minor)\n",
    "```\n",
    "  def gradient_descent(self, X, y, learning_rate):\n",
    "  ```\n",
    "\n",
    "  Could be more specific:\n",
    "  ```\n",
    "  def gradient_descent_step(self, X, y, learning_rate):\n",
    "  ```\n",
    "\n",
    "  Since it performs one step, not the full gradient descent loop. But this is nitpicking!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8a82c-391c-48fe-a210-a060cace126d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
