{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b487fc3-3011-41e7-b023-3e21e40160f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea848628-7450-4f0a-bbe8-1bf8d738241f",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d6586-9a85-44ae-b684-1e9fcd93a0d9",
   "metadata": {},
   "source": [
    "##  Now use vector `w` (each `x_i` is a vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d83dd3-d758-414f-94a1-ec6c01ffcd13",
   "metadata": {},
   "source": [
    "* X: (n, m)\n",
    "* y: (n,)\n",
    "* w: (m,)\n",
    "* b: scalar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b8b0b-1e2e-4658-a817-1949d08c10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3], [2,3,4]])\n",
    "w = np.array([1,2,1])\n",
    "b = .1\n",
    "print(X.shape)\n",
    "print(len(X))\n",
    "print(w.shape)\n",
    "print()\n",
    "\n",
    "print(np.sum(X * w, axis=1) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5297e0-fafb-4857-8221-74e4e2c8f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X@w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a8ef6-d8a3-4e2b-932e-8ea5d2df608d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f73f3-b4e0-476f-bb79-81c76d87e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LinearRegression(3)\n",
    "yP = l.predict(X)\n",
    "y = np.array([0,1])\n",
    "print(yP, y)\n",
    "print(l.loss(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678a547c-1270-411d-8025-e4d926e812db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.T.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c13ea-a31e-468b-9e5e-e7e8b5f771d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.T)\n",
    "print(yP - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7293c-108d-409e-b844-dfdac5618708",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.T.dot(yP-y) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7a498e-247f-4acc-86d0-03eb264ec313",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((yP-y) @ X / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ed02c-df6a-44e2-865a-544dbdb60411",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y-yP)\n",
    "print((y-yP).dot(y-yP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfae9252-c0ca-4b27-a190-82194b40ce9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((y-yP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4a6865-9fa3-4bac-8fb7-3803f6725e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.T * (y-yP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d8b35-7ce8-4996-8945-21bd7eadd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(X.T * (y-yP), axis=1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfbccc-f02f-4e00-9e2b-2fd7faf5d55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.T.dot(y-yP)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d43169c-5c95-4587-aad3-9866d623502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class LinearRegression:\n",
    "    def __init__(self, m):\n",
    "        self.w = np.random.randn(m)\n",
    "        self.b = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.w + self.b\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        n = len(X)\n",
    "        yP = self.predict(X)\n",
    "        return (y-yP).dot(y-yP) / (2 * n)\n",
    "\n",
    "    def backprop(self, X, y, learning_rate):\n",
    "        n = len(X)\n",
    "        yP = self.predict(X)\n",
    "        dldw = X.T.dot(yP - y) / n\n",
    "        dldb = np.sum(yP - y) / n\n",
    "        \n",
    "        self.w -= learning_rate * dldw\n",
    "        self.b -= learning_rate * dldb\n",
    "\n",
    "    def fit(self, X, y, steps, learning_rate = 0.01):\n",
    "        for i in range(steps):\n",
    "            self.backprop(X, y, learning_rate)\n",
    "            if (i % 50 == 0):\n",
    "                print(\"Steps:\", i)\n",
    "                print(f\"w: {self.w}, b: {self.b}, yP: {self.predict(X)}, loss: {self.loss(X,y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a788bb5-d29c-4ca3-9468-28ebad32bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(2,3)\n",
    "y = np.random.randn(2)\n",
    "l = LinearRegression(3)\n",
    "print(f\"X={X}, y={y}\")\n",
    "l.fit(X, y, 501, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336dbbb3-aa14-4d1b-b332-6102d031be53",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3e150a-c143-45f3-bd64-7fade62a1182",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "üí° Suggestions for Improvement\n",
    "\n",
    "  1. Simpler prediction (optional but cleaner)\n",
    "```\n",
    "  def predict(self, X):\n",
    "      # Current: np.sum(X * self.w, axis=1) + self.b\n",
    "      # Simpler: use @ for matrix multiplication\n",
    "      return X @ self.w + self.b  # ‚úÖ More readable\n",
    "```\n",
    "  2. Consider adding feature normalization\n",
    "```\n",
    "  # In __init__ or fit()\n",
    "  self.mean = None\n",
    "  self.std = None\n",
    "\n",
    "  def fit(self, X, y, steps, learning_rate=0.01, normalize=True):\n",
    "      if normalize:\n",
    "          self.mean = X.mean(axis=0)\n",
    "          self.std = X.std(axis=0)\n",
    "          X = (X - self.mean) / self.std\n",
    "      # ... rest of training\n",
    "```\n",
    "  3. Return loss history (useful for debugging)\n",
    "```\n",
    "  def fit(self, X, y, steps, learning_rate=0.01):\n",
    "      losses = []\n",
    "      for i in range(steps):\n",
    "          self.backprop(X, y, learning_rate)\n",
    "          if i % 50 == 0:\n",
    "              loss = self.loss(X, y)\n",
    "              losses.append(loss)\n",
    "              print(f\"Step {i}: loss = {loss:.4f}\")\n",
    "      return losses  # Can plot this later\n",
    "```\n",
    "  4. Add convergence check (early stopping)\n",
    "```\n",
    "  def fit(self, X, y, steps, learning_rate=0.01, tol=1e-6):\n",
    "      prev_loss = float('inf')\n",
    "      for i in range(steps):\n",
    "          self.backprop(X, y, learning_rate)\n",
    "          curr_loss = self.loss(X, y)\n",
    "\n",
    "          if abs(prev_loss - curr_loss) < tol:\n",
    "              print(f\"Converged at step {i}\")\n",
    "              break\n",
    "          prev_loss = curr_loss\n",
    "```\n",
    "  üìä How to Test It Works\n",
    "```\n",
    "  # Generate simple test data\n",
    "  from sklearn.datasets import make_regression\n",
    "  X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)\n",
    "\n",
    "  # Your implementation\n",
    "  model = LinearRegression(m=3)\n",
    "  model.fit(X, y, steps=1000, learning_rate=0.01)\n",
    "\n",
    "  # Compare with sklearn\n",
    "  from sklearn.linear_model import LinearRegression as SklearnLR\n",
    "  sklearn_model = SklearnLR()\n",
    "  sklearn_model.fit(X, y)\n",
    "\n",
    "  print(\"\\nYour weights:\", model.w)\n",
    "  print(\"Sklearn weights:\", sklearn_model.coef_)\n",
    "  print(\"\\nYour bias:\", model.b)\n",
    "  print(\"Sklearn bias:\", sklearn_model.intercept_)\n",
    "\n",
    "  # Should be similar (not exact due to different optimizers)\n",
    "```\n",
    "  üéØ Assessment Results\n",
    "\n",
    "  What this tells me about your current level:\n",
    "\n",
    "  Strengths:\n",
    "\n",
    "  - ‚úÖ Understand gradient descent conceptually\n",
    "  - ‚úÖ Know vectorization with numpy\n",
    "  - ‚úÖ Can structure code cleanly\n",
    "  - ‚úÖ Remember MSE loss formula\n",
    "  - ‚úÖ Understand backpropagation concept\n",
    "\n",
    "  Areas to Practice:\n",
    "\n",
    "  - ‚ö†Ô∏è Small syntax errors (typos: b vs self.b, x vs X)\n",
    "  - ‚ö†Ô∏è Took >1hr (goal: 20-30min for linear regression)\n",
    "  - üí° Could simplify with @ operator instead of np.sum(... axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544adfe-319d-42ab-85b7-2367b9795038",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Generate simple test data\n",
    "  from sklearn.datasets import make_regression\n",
    "  X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)\n",
    "\n",
    "  # Your implementation\n",
    "  model = LinearRegression(m=3)\n",
    "  model.fit(X, y, steps=1000, learning_rate=0.01)\n",
    "\n",
    "  # Compare with sklearn\n",
    "  from sklearn.linear_model import LinearRegression as SklearnLR\n",
    "  sklearn_model = SklearnLR()\n",
    "  sklearn_model.fit(X, y)\n",
    "\n",
    "  print(\"\\nYour weights:\", model.w)\n",
    "  print(\"Sklearn weights:\", sklearn_model.coef_)\n",
    "  print(\"\\nYour bias:\", model.b)\n",
    "  print(\"Sklearn bias:\", sklearn_model.intercept_)\n",
    "\n",
    "  # Should be similar (not exact due to different optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df8ea1-7e4f-415a-af9c-e9539c0fbeef",
   "metadata": {},
   "source": [
    "\n",
    "  Today (Rest of Day 1):\n",
    "\n",
    "  1. Try Logistic Regression (give yourself 1 hour max)\n",
    "    - Will likely be similar difficulty\n",
    "    - Focus on sigmoid function and binary cross-entropy\n",
    "  2. Compare with solution in ML-Coding-Questions.md\n",
    "  3. Note what took longest - that's your focus area\n",
    "\n",
    "  Day 2:\n",
    "\n",
    "  1. Review gradient formulas for common algorithms\n",
    "  2. Practice matrix operation patterns\n",
    "  3. Identify 3-5 more algorithms to implement this week\n",
    "\n",
    "  This Week:\n",
    "\n",
    "  Implement these from scratch (get each under 30 min):\n",
    "  1. K-Nearest Neighbors (easier - good warmup)\n",
    "  2. K-Means (medium)\n",
    "  3. Decision Tree (harder - optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c8efc5-abb9-4d18-936b-a833f47754b2",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d00336-1e02-4564-9e9e-d42b5dbd4dc4",
   "metadata": {},
   "source": [
    "* Sigmoid function: sigmoid(z) = 1 / (1 + e^(-z))\n",
    "* z = X @ w + b\n",
    "* **Cross entropy loss**: -(y * log(yP) + (1-y) * log(1-yP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7829f-dedb-40f8-8aaf-e497d3b4ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2,3], [8,4,1]])\n",
    "l = LogisticRegression(3)\n",
    "y = np.array([0,1])\n",
    "print(l.predict(X))\n",
    "\n",
    "yP = l.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a0750-8bdc-48ff-a462-42551f5ef52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([0,0])\n",
    "print(yP, y)\n",
    "print(l.loss(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50a25e9b-f782-4b7b-895e-343fede6080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, m):\n",
    "        self.w = np.random.randn(m)\n",
    "        self.b = 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = X @ self.w + self.b\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        yP = self.predict(X)\n",
    "        return -(y.dot(np.log(yP)) + (1-y).dot(np.log(1-yP))) / len(X)\n",
    "\n",
    "    def back_propagate(self, X, y, learning_rate):\n",
    "        n = len(X)\n",
    "        yP = self.predict(X)\n",
    "        dldw = X.T @ (yP - y) / n\n",
    "        dldb = np.sum(yP - y) / n\n",
    "        self.w -= dldw * learning_rate\n",
    "        self.b -= dldb * learning_rate\n",
    "\n",
    "    def fit(self, X, y, steps, learning_rate=0.01):\n",
    "        for i in range(steps):\n",
    "            self.back_propagate(X, y, learning_rate)\n",
    "            if i % 50 == 0:\n",
    "                loss = self.loss(X, y)\n",
    "                print(f\"step: {i}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52639f9a-8c4c-402c-95a0-981a6a461627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 1.6254716734663455\n",
      "step: 50, loss: 1.376383049263897\n",
      "step: 100, loss: 1.154138318236018\n",
      "step: 150, loss: 0.9645073869374869\n",
      "step: 200, loss: 0.8126238986235069\n",
      "step: 250, loss: 0.6981746779318883\n",
      "step: 300, loss: 0.6147518196331508\n",
      "step: 350, loss: 0.5538184009938333\n",
      "step: 400, loss: 0.508107030715272\n",
      "step: 450, loss: 0.4725406252261659\n",
      "step: 500, loss: 0.44386114419466205\n",
      "step: 550, loss: 0.42002873270994057\n",
      "step: 600, loss: 0.39975412097323904\n",
      "step: 650, loss: 0.382198348028718\n",
      "step: 700, loss: 0.36679314010787156\n",
      "step: 750, loss: 0.35313596045417206\n",
      "step: 800, loss: 0.34092881492619653\n",
      "[1 0 1 0 1 0 0 0 1 0] [0.79698729 0.59881974 0.79536481 0.12713059 0.39177987 0.01729298\n",
      " 0.31109396 0.10481732 0.68050655 0.07809064]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(10, 8)\n",
    "l = LogisticRegression(8)\n",
    "y = np.random.randint(0, 2, size=10)\n",
    "l.fit(X, y, 801)\n",
    "print(y, l.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41fa3ae-34c5-4b9d-b1ad-269b7e7d0e02",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9e3f2-e869-4d31-b652-f982054bec48",
   "metadata": {},
   "source": [
    "  üîç Issues & Improvements\n",
    "\n",
    "  1. Critical Bug in Gradient Calculation ‚ö†Ô∏è\n",
    "```\n",
    "  # Your code:\n",
    "  dldw = (yP - y) @ X / n  # ‚ùå WRONG ORDER!\n",
    "\n",
    "  # Should be:\n",
    "  dldw = X.T @ (yP - y) / n  # ‚úÖ CORRECT\n",
    "```\n",
    "  Why this matters:\n",
    "  - (yP - y) has shape (n,) - n samples\n",
    "  - X has shape (n, m) - n samples, m features\n",
    "  - (yP - y) @ X gives shape (m,) ‚úÖ (accidentally correct!)\n",
    "  - BUT conceptually wrong - should be X.T @ (yP - y)\n",
    "\n",
    "  Your code works because of numpy broadcasting, but in an interview this would be flagged as conceptually incorrect.\n",
    "\n",
    "  Correct version:\n",
    "  ```\n",
    "  def back_propagate(self, X, y, learning_rate):\n",
    "      n = len(X)\n",
    "      yP = self.predict(X)\n",
    "      dldw = X.T @ (yP - y) / n  # X transpose times error\n",
    "      dldb = np.sum(yP - y) / n\n",
    "      self.w -= dldw * learning_rate\n",
    "      self.b -= dldb * learning_rate\n",
    "  ```\n",
    "\n",
    "  2. Loss Function - Missing Average\n",
    "```\n",
    "  # Your code:\n",
    "  def loss(self, X, y):\n",
    "      yP = self.predict(X)\n",
    "      return -(y.dot(np.log(yP)) + (1-y).dot(np.log(1-yP)))\n",
    "      # ‚ùå This is the TOTAL loss, not AVERAGE\n",
    "\n",
    "  # Should be:\n",
    "  def loss(self, X, y):\n",
    "      n = len(X)\n",
    "      yP = self.predict(X)\n",
    "      return -(y.dot(np.log(yP)) + (1-y).dot(np.log(1-yP))) / n\n",
    "      # ‚úÖ Now it's Mean Binary Cross-Entropy\n",
    "```\n",
    "  Why it matters:\n",
    "  - Without averaging, loss depends on dataset size\n",
    "  - Can't compare losses across different batch sizes\n",
    "  - Standard practice is to report average loss\n",
    "\n",
    "  3. Numerical Stability Issue (Important!)\n",
    "```\n",
    "  # Your code:\n",
    "  return -(y.dot(np.log(yP)) + (1-y).dot(np.log(1-yP)))\n",
    "  # ‚ö†Ô∏è Problem: log(0) = -inf if yP = 0 or yP = 1\n",
    "\n",
    "  # Better:\n",
    "  def loss(self, X, y):\n",
    "      n = len(X)\n",
    "      yP = self.predict(X)\n",
    "      epsilon = 1e-15  # Small value to prevent log(0)\n",
    "      yP = np.clip(yP, epsilon, 1 - epsilon)  # Clip predictions\n",
    "      return -(y.dot(np.log(yP)) + (1-y).dot(np.log(1-yP))) / n\n",
    "```\n",
    "  Why: Extreme predictions (0 or 1) cause log(0) ‚Üí numerical errors\n",
    "\n",
    "  4. Same Constructor Issue (as Linear Regression)\n",
    "```\n",
    "  # Your code:\n",
    "  def __init__(self, m):  # ‚ö†Ô∏è Manual feature count\n",
    "\n",
    "  # Better:\n",
    "  def __init__(self):\n",
    "      self.w = None\n",
    "      self.b = None\n",
    "\n",
    "  def fit(self, X, y, n_iterations=1000, learning_rate=0.01):\n",
    "      if self.w is None:\n",
    "          n_features = X.shape[1]\n",
    "          self.w = np.random.randn(n_features) * 0.01  # Small init\n",
    "          self.b = 0\n",
    "      # ... rest of code\n",
    "```\n",
    "  5. Weight Initialization (Minor but good practice)\n",
    "```\n",
    "  # For logistic regression, use smaller initial weights:\n",
    "  self.w = np.random.randn(m) * 0.01  # Scale down\n",
    "  # or\n",
    "  self.w = np.zeros(m)  # Start from zero\n",
    "\n",
    "  # Large initial weights ‚Üí extreme sigmoid values ‚Üí slow learning\n",
    "```\n",
    "  ---\n",
    "  üìä Assessment - Day 1 Complete!\n",
    "\n",
    "  Time Analysis:\n",
    "\n",
    "  - Linear Regression: 1 hour\n",
    "  - Logistic Regression: 1 hour\n",
    "  - Looked up: Cross-entropy formula ‚úÖ\n",
    "  - Struggled with: Computing derivative ‚ö†Ô∏è\n",
    "\n",
    "  What This Tells Me:\n",
    "\n",
    "  ‚úÖ Strong Areas:\n",
    "  - Numpy operations\n",
    "  - Sigmoid function (got it right!)\n",
    "  - Code structure (consistent pattern)\n",
    "  - Loss function (conceptually correct)\n",
    "\n",
    "  ‚ö†Ô∏è Rusty Areas:\n",
    "  1. Gradient derivation - This is where you lost time\n",
    "  2. Matrix dimensions - (yP - y) @ X vs X.T @ (yP - y)\n",
    "  3. Loss function details - Forgot averaging\n",
    "\n",
    "  üéØ Key Insight:\n",
    "  The gradient for logistic regression is beautiful: it simplifies to exactly X.T @ (yP - y) / n, same form as linear regression! This is one of those \"aha\"\n",
    "   moments in ML.\n",
    "\n",
    "  ---\n",
    "  üßÆ The Math (For Your Reference)\n",
    "\n",
    "  Binary Cross-Entropy Loss:\n",
    "\n",
    "  L = -(1/n) * Œ£[y*log(≈∑) + (1-y)*log(1-≈∑)]\n",
    "\n",
    "  Gradient (the tricky part):\n",
    "\n",
    "  dL/dw = (1/n) * X^T * (≈∑ - y)\n",
    "\n",
    "  Why it's beautiful:\n",
    "  - Same form as linear regression!\n",
    "  - The sigmoid and log cancel out perfectly\n",
    "  - This is why logistic regression is so elegant\n",
    "\n",
    "  Derivation (Quick):\n",
    "\n",
    "  dL/d≈∑ = -(y/≈∑ - (1-y)/(1-≈∑))\n",
    "  d≈∑/dz = ≈∑(1-≈∑)           [sigmoid derivative]\n",
    "  dz/dw = X\n",
    "\n",
    "  Chain rule: dL/dw = dL/d≈∑ * d≈∑/dz * dz/dw\n",
    "           = -(y/≈∑ - (1-y)/(1-≈∑)) * ≈∑(1-≈∑) * X\n",
    "           = (≈∑ - y) * X         [after algebra]\n",
    "\n",
    "  Memorize this result - you don't need to re-derive in interviews!\n",
    "\n",
    "  ---\n",
    "  üìù Day 1 Assessment Summary\n",
    "\n",
    "  Overall Performance:\n",
    "\n",
    "  Theory: 8/10 ‚úÖ\n",
    "  - Understand concepts\n",
    "  - Know the algorithms\n",
    "  - Slightly rusty on derivations\n",
    "\n",
    "  Implementation: 6.5/10 ‚ö†Ô∏è\n",
    "  - Can implement correctly\n",
    "  - Takes longer than interview speed (need 20-25 min, you're at 1 hour)\n",
    "  - Small bugs (matrix order, missing average)\n",
    "\n",
    "  Debugging: 7/10 ‚úÖ\n",
    "  - Good systematic approach\n",
    "  - Print statements help\n",
    "  - Need to catch dimension errors faster\n",
    "\n",
    "  Your Rust Profile:\n",
    "\n",
    "  | Area                 | Status   | Priority |\n",
    "  |----------------------|----------|----------|\n",
    "  | Numpy operations     | ‚úÖ Good   | Low      |\n",
    "  | Algorithm structure  | ‚úÖ Good   | Low      |\n",
    "  | Loss functions       | üü° Rusty | Medium   |\n",
    "  | Gradient derivations | üî¥ Rusty | HIGH     |\n",
    "  | Matrix dimensions    | üü° Rusty | Medium   |\n",
    "  | Vectorization        | ‚úÖ Good   | Low      |\n",
    "\n",
    "  ---\n",
    "  üéØ Your Focus Areas This Week\n",
    "\n",
    "  Based on Day 1 assessment:\n",
    "\n",
    "  High Priority:\n",
    "\n",
    "  1. Gradient formulas - Memorize common ones\n",
    "    - Linear Regression: X.T @ (yP - y) / n\n",
    "    - Logistic Regression: X.T @ (yP - y) / n (same!)\n",
    "    - These appear in 80% of ML algorithms\n",
    "  2. Matrix dimensions - Build intuition fast\n",
    "    - X: (n, m) - n samples, m features\n",
    "    - y: (n,) - n labels\n",
    "    - w: (m,) - m weights\n",
    "    - X.T @ (yP - y): (m, n) @ (n,) = (m,) ‚úÖ\n",
    "\n",
    "  Medium Priority:\n",
    "\n",
    "  3. Loss functions - Remember to average\n",
    "  4. Numerical stability - Clipping, epsilon tricks\n",
    "\n",
    "  Low Priority (You're Good):\n",
    "\n",
    "  5. Code structure\n",
    "  6. Basic numpy operations\n",
    "\n",
    "  ---\n",
    "  üìö Recommended Practice (Rest of Week 1)\n",
    "\n",
    "  Day 2 (Tomorrow):\n",
    "\n",
    "  1. Morning (1 hour): Re-implement both algorithms from scratch\n",
    "    - Goal: 30 minutes each\n",
    "    - Don't look at your code\n",
    "    - Focus on getting gradients right first try\n",
    "  2. Afternoon (1 hour): Review gradient formulas\n",
    "    - Write them down on paper\n",
    "    - Understand why they simplify\n",
    "    - Memorize the patterns\n",
    "\n",
    "  Days 3-7:\n",
    "\n",
    "  Continue with Week 1 plan:\n",
    "  - Fast.AI lessons (at 1.5x speed)\n",
    "  - Implement K-NN (easier, confidence builder)\n",
    "  - Start image classification project\n",
    "  - Maybe implement K-Means if time\n",
    "\n",
    "  ---\n",
    "  üí° Interview Context\n",
    "\n",
    "  In a 45-minute ML coding interview:\n",
    "  - You'd get 20-25 min for Logistic Regression\n",
    "  - Need to code + test in that time\n",
    "  - Your 1 hour ‚Üí need to speed up ~2-3x\n",
    "\n",
    "  Good news:\n",
    "  - With 5-10 more algorithm implementations, you'll hit interview speed\n",
    "  - Your foundation is solid (no conceptual gaps)\n",
    "  - Just need muscle memory\n",
    "\n",
    "  ---\n",
    "  ‚úÖ Day 1 Complete - Well Done!\n",
    "\n",
    "  What you proved today:\n",
    "  1. ‚úÖ You can implement ML algorithms from scratch\n",
    "  2. ‚úÖ Your numpy skills are solid\n",
    "  3. ‚úÖ You understand the concepts\n",
    "  4. ‚ö†Ô∏è Need to speed up (but that's just practice!)\n",
    "\n",
    "  Confidence check:\n",
    "  - You got both algorithms correct (with minor bugs)\n",
    "  - You can figure things out when stuck\n",
    "  - You're asking the right questions (looking up cross-entropy is smart!)\n",
    "\n",
    "  You're right on track! The assessment did exactly what it should - identified that you need to:\n",
    "  1. Practice gradient derivations (speed them up)\n",
    "  2. Build pattern recognition (matrix dimensions)\n",
    "  3. Implement more algorithms (build muscle memory)\n",
    "\n",
    "  Tomorrow, try re-implementing these without looking at your code. You'll probably do it in 30-40 minutes each, which will be a confidence boost! üöÄ\n",
    "\n",
    "  ---\n",
    "  Would you like me to:\n",
    "  1. Give you a quick reference sheet of gradient formulas to memorize?\n",
    "  2. Suggest which algorithm to tackle next (K-NN is a good confidence builder)?\n",
    "  3. Review your Week 1 schedule based on today's assessment?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa6cde-9856-4768-967f-e13f2c8b6b2c",
   "metadata": {},
   "source": [
    "# Retro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f626b1-873c-4c34-830c-b4f82ed4edb1",
   "metadata": {},
   "source": [
    "## what was difficult\n",
    "\n",
    "* Figure out how to tranfer matrix operations into numpy\n",
    "* Figure out the right dimensions of each operand\n",
    "* Taking derivitive when matrices are involved, and transfer that into numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93307c-27c0-42d7-8a22-007213d484b7",
   "metadata": {},
   "source": [
    "## what was forgotten\n",
    "\n",
    "* `b` is a scalar or a vector\n",
    "* dimension of `w` and `X`\n",
    "* Logistic regression loss function\n",
    "* Compute derivitive of the cross entropy loss took quite some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20af17-6abc-4a7a-9a06-5c6d444c2168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
